{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from get_class_weights import ENet_weighing, median_frequency_balancing\n",
    "import augmentation\n",
    "import viewer\n",
    "from labels import trainId2label\n",
    "from load_pretrained_weights import load_pretrained_weights\n",
    "\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = 'segmentation'\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)\n",
    "    print ('Successfully create directory.')\n",
    "else: \n",
    "    print ('Directory exists.')\n",
    "    \n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(tmp_dir, 'training.log'), \n",
    "    format='%(asctime)s %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class index with 0~18, 19 represents others\n",
    "num_class = 20\n",
    "weighting = 'ENet'\n",
    "combine_dataset = False\n",
    "\n",
    "tf_logdir = './log'\n",
    "\n",
    "train_dir = '../data/leftImg8bit/train'\n",
    "val_dir = '../data/leftImg8bit/val'\n",
    "test_dir = '../data/leftImg8bit/test'\n",
    "\n",
    "gt_train_dir = '../data/gtFine/train'\n",
    "gt_val_dir = '../data/gtFine/val'\n",
    "gt_test_dir = '../data/gtFine/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_dir(root):\n",
    "    file_list = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            file_list.append(os.path.join(path, name))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_train_files = walk_dir(train_dir)\n",
    "image_train_files = sorted(image_train_files)\n",
    "gt_train_files = walk_dir(gt_train_dir)\n",
    "gt_train_files = [gt_train_file for gt_train_file in gt_train_files \\\n",
    "            if gt_train_file.endswith('_labelTrainIds.png')]\n",
    "gt_train_files = sorted(gt_train_files)\n",
    "print ('Read {} training images.'.format(len(image_train_files)))\n",
    "print ('Read {} training groudtruth lables'.format(len(gt_train_files)))\n",
    "assert len(image_train_files) == len(gt_train_files)\n",
    "\n",
    "image_val_files = walk_dir(val_dir)\n",
    "image_val_files = sorted(image_val_files)\n",
    "gt_val_files = walk_dir(gt_val_dir)\n",
    "gt_val_files = [gt_val_file for gt_val_file in gt_val_files \\\n",
    "                if gt_val_file.endswith('_labelTrainIds.png')]\n",
    "gt_val_files = sorted(gt_val_files)\n",
    "print ('Read {} validation images.'.format(len(image_val_files)))\n",
    "print ('Read {} validation groudtruth lables.'.format(len(gt_val_files)))\n",
    "assert len(image_val_files) == len(gt_val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if combine_dataset:\n",
    "    image_train_files += image_val_files\n",
    "    gt_train_files += gt_val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weighting == 'MFB':\n",
    "    class_weights = median_frequency_balancing(\n",
    "        gt_train_files, num_classes=num_class)\n",
    "    print ('Median_frequency_balancing class weights is')\n",
    "    print (class_weights)\n",
    "elif weighting == 'ENet':\n",
    "    if os.path.isfile('enet_class_weights.npy'):\n",
    "        class_weights = np.load('enet_class_weights.npy')\n",
    "    else:\n",
    "        class_weights = ENet_weighing(\n",
    "            gt_train_files, num_classes=num_class)\n",
    "        np.save('enet_class_weights.npy', class_weights)\n",
    "        print ('Save Enet_class_weights')\n",
    "        print ('ENet class weights is')\n",
    "        \n",
    "class_weights[19] = 0\n",
    "print ('Id            Class                Weight')\n",
    "for i in range(int(class_weights.shape[0])):\n",
    "    print ('{:2}            {:15}  {:>10.6}'.\n",
    "          format(i, trainId2label[i].name, class_weights[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(image_train_files, gt_train_files, \n",
    "                   image_val_files, gt_val_files, image_size):\n",
    "\n",
    "    num_worker = 8\n",
    "    def _parse_function(image_file, gt_file, image_size=image_size):\n",
    "        image_string = tf.read_file(image_file)\n",
    "        gt_string = tf.read_file(gt_file)\n",
    "\n",
    "        image = tf.image.decode_image(image_string)\n",
    "        # Need to set shape\n",
    "        # https://github.com/tensorflow/tensorflow/issues/8551\n",
    "        image.set_shape(shape=(1024, 2048, 3))\n",
    "        image = tf.image.resize_images(image, image_size)\n",
    "        #image = tf.image.per_image_standardization(image)\n",
    "\n",
    "        gt = tf.image.decode_image(gt_string)\n",
    "        gt.set_shape(shape=(1024, 2048, 1))\n",
    "        gt = tf.image.resize_images(\n",
    "            gt, image_size, tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        \n",
    "        image, gt = augmentation.random_flip_left_right(image, gt)\n",
    "        \n",
    "        image = image - \\\n",
    "            np.array([103.939, 116.779, 123.68], dtype=np.float32)\n",
    "\n",
    "        return image, gt\n",
    "\n",
    "    image_train = tf.data.Dataset.from_tensor_slices(\n",
    "        tf.constant(image_train_files))\n",
    "    gt_train = tf.data.Dataset.from_tensor_slices(\n",
    "        tf.constant(gt_train_files))\n",
    "    train_dataset = tf.data.Dataset.zip((image_train, gt_train))\n",
    "    # https://stackoverflow.com/questions/44132307/tf-contrib-data-dataset-repeat-with-shuffle-notice-epoch-end-mixed-epochs\n",
    "    train_dataset = train_dataset.shuffle(len(image_train_files))\n",
    "    train_dataset = train_dataset.map(\n",
    "        _parse_function, num_parallel_calls=num_worker\n",
    "    )\n",
    "    train_dataset = train_dataset.repeat()\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.constant(image_val_files), tf.constant(gt_val_files)))\n",
    "    val_dataset = val_dataset.map(\n",
    "        _parse_function, num_parallel_calls=num_worker\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(sess, net, max_step, learning_rate, print_every=0):\n",
    "    \n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    mean_iou_list = []\n",
    "    \n",
    "    # Training step\n",
    "    for step in tqdm(range(max_step)):\n",
    "        try:\n",
    "            # Reset metrics counters\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            training_loss, _, _ = sess.run(\n",
    "                [net.loss, net.metrics_update, net.opt], \n",
    "                feed_dict={\n",
    "                    net.input_learning_rate: learning_rate\n",
    "                })\n",
    "            accuracy, mean_iou = sess.run(\n",
    "                [net.accuracy, net.mean_iou]\n",
    "            )\n",
    "            loss_list.append(training_loss)\n",
    "            accuracy_list.append(accuracy)\n",
    "            mean_iou_list.append(mean_iou)\n",
    "            # For print\n",
    "            print_i = 0\n",
    "            if print_every > 0:\n",
    "                print_i = print_i + 1\n",
    "                if print_i % print_every == 0:\n",
    "                    print_i = 0\n",
    "                    print ('Training loss is {}, accuracy is {}, mean iou is {}'\\\n",
    "                           .format(training_loss, accuracy, mean_iou))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print (\"Training iterator emptied.\")\n",
    "            break    \n",
    "    print ('Mean loss in epoch is {}, accuracy is {}, mean iou is {}'.format(\n",
    "        np.mean(loss_list), np.mean(accuracy_list), np.mean(mean_iou_list)\n",
    "    ))\n",
    "    return loss_list, accuracy_list, mean_iou_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(sess, net, max_step, val_init):\n",
    "    val_loss_list = []\n",
    "    # Validation step\n",
    "    # Reset validation dateset\n",
    "    sess.run(val_init)\n",
    "    # Reset matrics counters\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    val_step = 0\n",
    "    for step in tqdm(range(max_step)):\n",
    "        try:\n",
    "            val_loss, _ = sess.run(\n",
    "                [net.val_loss, net.val_metrics_update]\n",
    "            )\n",
    "            val_loss_list.append(val_loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print ('Validation iterator emptied.')\n",
    "    # Aggregate final results\n",
    "    accuracy, mean_iou = sess.run(\n",
    "        [net.val_accuracy, net.val_mean_iou]\n",
    "    )\n",
    "    print ('Validation accuracy is {}, mean iou is {}'.format(\n",
    "        accuracy, mean_iou\n",
    "    ))\n",
    "    return val_loss_list, accuracy, mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_header():\n",
    "    formats = '{:>8.6}' + '{:>10.8}  ' * 5 + '{}'\n",
    "    logging.info(formats.format(\n",
    "        'epoch', 'loss', 'accuracy', 'mean_iou',\n",
    "        'val_loss', 'val_acc', 'val_mean_iou'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from train import LinkNet\n",
    "\n",
    "label_channels = 20\n",
    "input_dims = [512, 1024]\n",
    "batch_size = 16\n",
    "\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.1\n",
    "lr_decay_every = 60\n",
    "tf.reset_default_graph()\n",
    "\n",
    "restore_training = False\n",
    "start_epoch = 0\n",
    "epochs = 180\n",
    "train_steps = int(math.ceil(2975 / batch_size))\n",
    "val_steps = int(math.ceil(500 / batch_size / 1.5))\n",
    "\n",
    "# Get batch data\n",
    "with tf.name_scope('input_pipe_line'):\n",
    "    with tf.device('/cpu:0'):\n",
    "        train_dataset, val_dataset = input_pipeline(\n",
    "            image_train_files, gt_train_files, image_val_files, \n",
    "            gt_val_files, input_dims\n",
    "        )\n",
    "\n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        train_dataset = train_dataset.prefetch(1)\n",
    "        train_iterator = train_dataset.make_one_shot_iterator()\n",
    "        train_batch = train_iterator.get_next()\n",
    "\n",
    "        val_dataset = val_dataset.batch(int(batch_size * 1.5))\n",
    "        val_dataset = val_dataset.prefetch(1)\n",
    "        val_iterator = val_dataset.make_initializable_iterator()\n",
    "        val_init = val_iterator.initializer\n",
    "        val_batch = val_iterator.get_next()\n",
    "\n",
    "net = LinkNet(\n",
    "    input_dims, label_channels, class_weights, \n",
    "    input_method='dataset_api', inputs=train_batch, \n",
    "    inputs_val=val_batch)\n",
    "\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "graph_writer = tf.summary.FileWriter('log/train', sess.graph)\n",
    "\n",
    "if restore_training:\n",
    "    net.restore(sess, os.path.join(tmp_dir, \n",
    "        'model-e{}.ckpt'.format(start_epoch - 1)))\n",
    "    train_metrics = np.load(\n",
    "        os.path.join(tmp_dir, 'train_metrics-e{}.npy'.\\\n",
    "                     format(start_epoch - 1))).item()\n",
    "    val_metrics = np.load(\n",
    "        os.path.join(tmp_dir, 'train_metrics-e{}.npy'.\\\n",
    "                     format(start_epoch - 1))).item()\n",
    "else:\n",
    "    load_pretrained_weights\n",
    "    train_metrics = {}\n",
    "    val_metrics = {}\n",
    "\n",
    "metrics = ['loss', 'accuracy', 'mean_iou']\n",
    "def store_metrics(dictionary, metrics, values):\n",
    "    for metric, value in zip(metrics, values):\n",
    "        dictionary.setdefault(metric, []).append(value)\n",
    "\n",
    "logging_header()\n",
    "for epoch in range(epochs):\n",
    "    if start_epoch + epoch != 0 and (start_epoch + epoch) % lr_decay_every == 0:\n",
    "        learning_rate = learning_rate * learning_rate_decay\n",
    "        logging.info(\n",
    "            'Learning rate change to {}.'.format(learning_rate)\n",
    "        )\n",
    "    print ('Epoch {} begin.'.format(start_epoch + epoch))\n",
    "    loss_list, accuracy_list, mean_iou_list = train_step(\n",
    "        sess, net, train_steps, learning_rate)\n",
    "    val_loss_list, val_accuracy, val_mean_iou = val_step(\n",
    "        sess, net, val_steps, val_init)\n",
    "    \n",
    "    store_metrics(\n",
    "        train_metrics, metrics, [np.mean(loss_list), \n",
    "        np.mean(accuracy_list), np.mean(mean_iou_list)]\n",
    "    )\n",
    "    store_metrics(\n",
    "        val_metrics, metrics, [np.mean(val_loss_list),\n",
    "        val_accuracy, val_mean_iou]\n",
    "    )\n",
    "    net.save(sess, os.path.join(\n",
    "        tmp_dir, 'model-e{}.ckpt'.format(start_epoch + epoch)))\n",
    "    \n",
    "    formats = '{:8d}' + '{:>10.6}  ' * 6\n",
    "    logging.info(formats.format(\n",
    "        start_epoch + epoch, np.mean(loss_list), \n",
    "        np.mean(accuracy_list), np.mean(mean_iou_list), \n",
    "        np.mean(val_loss_list), val_accuracy, val_mean_iou))\n",
    "    np.save(os.path.join(tmp_dir, \n",
    "            'train_metrics-e{}.npy'.format(start_epoch + epoch)), \n",
    "            np.array(train_metrics))\n",
    "    np.save(os.path.join(tmp_dir, \n",
    "            'train_metrics-e{}.npy'.format(start_epoch, epoch)),\n",
    "            np.array(val_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import LinkNet\n",
    "\n",
    "label_channels = 20\n",
    "input_dims = [512, 1024]\n",
    "batch_size = 16\n",
    "\n",
    "learning_rate = 0.01\n",
    "tf.reset_default_graph()\n",
    "\n",
    "epochs = 30\n",
    "train_steps = int(math.ceil(2975 / batch_size))\n",
    "val_steps = int(math.ceil(500 / batch_size / 1.5))\n",
    "\n",
    "# Get batch data\n",
    "with tf.name_scope('input_pipe_line'):\n",
    "    train_dataset, val_dataset = input_pipeline(\n",
    "        image_train_files, gt_train_files, image_val_files, \n",
    "        gt_val_files, input_dims\n",
    "    )\n",
    "\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    train_iterator = train_dataset.make_one_shot_iterator()\n",
    "    train_batch = train_iterator.get_next()\n",
    "    \n",
    "    val_dataset = val_dataset.batch(int((batch_size) * 1.5))\n",
    "    val_iterator = val_dataset.make_initializable_iterator()\n",
    "    val_init = val_iterator.initializer\n",
    "    val_batch = val_iterator.get_next()\n",
    "\n",
    "net = LinkNet(\n",
    "    input_dims, label_channels, class_weights, \n",
    "    input_method='dataset_api', inputs=train_batch, \n",
    "    inputs_val=val_batch)\n",
    "\n",
    "sess = tf.Session()\n",
    "net.restore(sess, os.path.join(tmp_dir, 'model-e59.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "viewer.predict_and_show(\n",
    "    10, image_val_files, gt_val_files, sess, net, tmp_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot label color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_ = plt.figure(figsize=(4, 8))\n",
    "ax_ = fig_.add_subplot(111)\n",
    "for i in range(20):\n",
    "    ax_.text(0, 19 - i, '{:^20}'.format(trainId2label[i].name), \n",
    "             bbox={\n",
    "                 'facecolor': np.array(trainId2label[i].color) / 256,\n",
    "                 'alpha': 0.5, 'pad': 5\n",
    "             })\n",
    "    ax_.axis([0, 10, 0, 20])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf15]",
   "language": "python",
   "name": "conda-env-tf15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
